{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Maxim location for all the months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*naamsestraat-35-maxim.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_maxim_df = pd.concat(df_list, ignore_index=True)\n",
    "full_maxim_df.shape\n",
    "\n",
    "del df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_maxim_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_maxim_df = full_maxim_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_maxim_df.reset_index(inplace=True)\n",
    "full_maxim_df.to_parquet('Dataset/full_maxim_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_timestamp</th>\n",
       "      <th>lamax</th>\n",
       "      <th>laeq</th>\n",
       "      <th>lceq</th>\n",
       "      <th>lcpeak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-28 08:20:00</td>\n",
       "      <td>58.400905</td>\n",
       "      <td>56.715493</td>\n",
       "      <td>67.821278</td>\n",
       "      <td>79.327515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-28 08:40:00</td>\n",
       "      <td>59.563057</td>\n",
       "      <td>57.756369</td>\n",
       "      <td>65.896815</td>\n",
       "      <td>77.866975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-02-28 09:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-02-28 09:20:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-02-28 09:40:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21401</th>\n",
       "      <td>2022-12-22 14:00:00</td>\n",
       "      <td>55.792917</td>\n",
       "      <td>53.752500</td>\n",
       "      <td>62.573792</td>\n",
       "      <td>75.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21402</th>\n",
       "      <td>2022-12-22 14:20:00</td>\n",
       "      <td>55.173917</td>\n",
       "      <td>53.262000</td>\n",
       "      <td>61.611900</td>\n",
       "      <td>74.158667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21403</th>\n",
       "      <td>2022-12-22 14:40:00</td>\n",
       "      <td>56.143417</td>\n",
       "      <td>54.098583</td>\n",
       "      <td>62.389817</td>\n",
       "      <td>75.100867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21404</th>\n",
       "      <td>2022-12-22 15:00:00</td>\n",
       "      <td>56.173750</td>\n",
       "      <td>54.176333</td>\n",
       "      <td>61.712333</td>\n",
       "      <td>74.699358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21405</th>\n",
       "      <td>2022-12-22 15:20:00</td>\n",
       "      <td>55.518324</td>\n",
       "      <td>53.508090</td>\n",
       "      <td>60.805663</td>\n",
       "      <td>73.957368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21406 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         result_timestamp      lamax       laeq       lceq     lcpeak\n",
       "0     2022-02-28 08:20:00  58.400905  56.715493  67.821278  79.327515\n",
       "1     2022-02-28 08:40:00  59.563057  57.756369  65.896815  77.866975\n",
       "2     2022-02-28 09:00:00        NaN        NaN        NaN        NaN\n",
       "3     2022-02-28 09:20:00        NaN        NaN        NaN        NaN\n",
       "4     2022-02-28 09:40:00        NaN        NaN        NaN        NaN\n",
       "...                   ...        ...        ...        ...        ...\n",
       "21401 2022-12-22 14:00:00  55.792917  53.752500  62.573792  75.210100\n",
       "21402 2022-12-22 14:20:00  55.173917  53.262000  61.611900  74.158667\n",
       "21403 2022-12-22 14:40:00  56.143417  54.098583  62.389817  75.100867\n",
       "21404 2022-12-22 15:00:00  56.173750  54.176333  61.712333  74.699358\n",
       "21405 2022-12-22 15:20:00  55.518324  53.508090  60.805663  73.957368\n",
       "\n",
       "[21406 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxim_df = pd.read_parquet('Dataset/full_maxim_df.parquet')\n",
    "maxim_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selecting night hours + Adding day of the week and holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katri\\AppData\\Local\\Temp\\ipykernel_29228\\2508151511.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  maxim_df.loc[:, 'night_scale'] = (maxim_df['result_timestamp'] - pd.Timedelta(hours=8)).dt.strftime('%d-%m-%Y %H:%M')\n",
      "C:\\Users\\katri\\AppData\\Local\\Temp\\ipykernel_29228\\2508151511.py:4: UserWarning: Parsing dates in %d-%m-%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  maxim_df['night_scale'] = pd.to_datetime(maxim_df['night_scale'])\n",
      "C:\\Users\\katri\\AppData\\Local\\Temp\\ipykernel_29228\\2508151511.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  maxim_df['night_scale'] = pd.to_datetime(maxim_df['night_scale'])\n",
      "C:\\Users\\katri\\AppData\\Local\\Temp\\ipykernel_29228\\2508151511.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  maxim_df.loc[:, 'night_hour'] = (maxim_df['night_scale'].dt.hour + maxim_df['night_scale'].dt.minute/60) - 11\n",
      "C:\\Users\\katri\\AppData\\Local\\Temp\\ipykernel_29228\\2508151511.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  maxim_df.loc[:, 'night'] = maxim_df['night_scale'].dt.strftime('%d-%m-%Y')\n",
      "C:\\Users\\katri\\AppData\\Local\\Temp\\ipykernel_29228\\2508151511.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  maxim_df[\"weekday\"] = maxim_df['night_scale'].dt.day_name()\n",
      "C:\\Users\\katri\\AppData\\Local\\Temp\\ipykernel_29228\\2508151511.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  maxim_df['Holiday']=is_holiday.map({True: 1, False:0})\n"
     ]
    }
   ],
   "source": [
    "maxim_df = maxim_df.loc[(maxim_df['result_timestamp'].dt.hour <= 7) | (maxim_df['result_timestamp'].dt.hour >= 19)]\n",
    "\n",
    "maxim_df.loc[:, 'night_scale'] = (maxim_df['result_timestamp'] - pd.Timedelta(hours=8)).dt.strftime('%d-%m-%Y %H:%M')\n",
    "maxim_df['night_scale'] = pd.to_datetime(maxim_df['night_scale'])\n",
    "   #night from monday to tuesday counted as monday\n",
    "maxim_df.loc[:, 'night_hour'] = (maxim_df['night_scale'].dt.hour + maxim_df['night_scale'].dt.minute/60) - 11\n",
    "maxim_df.loc[:, 'night'] = maxim_df['night_scale'].dt.strftime('%d-%m-%Y')\n",
    "\n",
    "maxim_df[\"weekday\"] = maxim_df['night_scale'].dt.day_name()\n",
    "holiday = [\"01-01-2022\", \"18-04-2022\", \"16-05-2022\", \"21-07-2022\", \"25-08-2022\", \"01-11-2022\", \"02-11-2022\",\"11-11-2022\", \"25-12-2022\"]\n",
    "is_holiday = maxim_df['night'].isin(holiday)\n",
    "maxim_df['Holiday']=is_holiday.map({True: 1, False:0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxim_df['result_timestamp'].dt.month.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = maxim_df[maxim_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3, 10, 11, 12])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df[\"result_timestamp\"].dt.month.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28, 27, 25, 26, 29, 30, 31,  1,  2, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23,  3,  4,  5,  6,  7,  8,  9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df['result_timestamp'].dt.day.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "# missing_df['result_timestamp'].map(lambda x: datetime.datetime(x.month, max(calendar.monthcalendar(x.year, x.month)[-1][:5])))\n",
    "\n",
    "missing_days_months = missing_df['result_timestamp'].apply(lambda x: x.strftime('%B-%d-Â´%H:%M')) \n",
    "for (date_time) in missing_days_months:\n",
    "    if datetime.strptime(date_time,'%B-%d-Â´%H:%M').hour <= 5 or datetime.strptime(date_time,'%B-%d-Â´%H:%M').hour >=19:\n",
    "        print(date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3, 10, 11, 12])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df['result_timestamp'].dt.month.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "oct_df = maxim_df[maxim_df['result_timestamp'].dt.month == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_oct_df = oct_df[oct_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_oct_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read Xior location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*xior.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                     usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser)  \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_xior_df = pd.concat(df_list, ignore_index=True)\n",
    "full_xior_df.shape\n",
    "\n",
    "del df_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above code: error dtype float\n",
    "Below: alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*xior.csv\") )\n",
    "full_xior_df = pd.concat(df_list, ignore_index=True)\n",
    "full_xior_df = full_xior_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_xior_df['result_timestamp'] = full_xior_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_xior_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_xior_df = full_xior_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_xior_df.reset_index(inplace=True)\n",
    "full_xior_df.to_parquet('Dataset/full_xior_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('Dataset/full_xior_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read Taste location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*taste.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_taste_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_taste_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_taste_df = full_taste_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_taste_df.reset_index(inplace=True)\n",
    "full_taste_df.to_parquet('Dataset/full_taste_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*taste.csv\") )\n",
    "full_taste_df = pd.concat(df_list, ignore_index=True)\n",
    "full_taste_df = full_taste_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "\n",
    "full_taste_df['result_timestamp'] = full_taste_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "full_taste_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_taste_df = full_taste_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_taste_df.reset_index(inplace=True)\n",
    "full_taste_df.to_parquet('Dataset/full_taste_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_taste_df = pd.read_parquet('Dataset/full_maxim_df.parquet')\n",
    "full_taste_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read Kapel location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*calvariekapel-ku-leuven.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_kapel_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_kapel_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_kapel_df = full_kapel_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_kapel_df.reset_index(inplace=True)\n",
    "full_kapel_df.to_parquet('Dataset/full_kapel_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*calvariekapel-ku-leuven.csv\") )\n",
    "full_kapel_df = pd.concat(df_list, ignore_index=True)\n",
    "full_kapel_df = full_kapel_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "\n",
    "full_kapel_df['result_timestamp'] = full_kapel_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_kapel_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_kapel_df =full_kapel_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_kapel_df.reset_index(inplace=True)\n",
    "full_kapel_df.to_parquet('Dataset/full_kapel_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read filosovia location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = '/Users/siucheung/School/Modern Data Analytics/project/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*filosovia.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_filo_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_filo_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_filo_df = full_filo_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_filo_df.reset_index(inplace=True)\n",
    "full_filo_df.to_parquet('Dataset/full_filo_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*filosovia.csv\"))\n",
    "full_filo_df = pd.concat(df_list, ignore_index=True)\n",
    "full_filo_df = full_filo_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "full_filo_df['result_timestamp'] =full_filo_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_filo_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_filo_df =full_filo_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_filo_df.reset_index(inplace=True)\n",
    "full_filo_df.to_parquet('Dataset/full_filo_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read 81 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = '/Users/siucheung/School/Modern Data Analytics/project/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*81.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_81_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_81_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_81_df = full_81_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_81_df.reset_index(inplace=True)\n",
    "full_81_df.to_parquet('Dataset/full_81_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*81.csv\"))\n",
    "full_81_df = pd.concat(df_list, ignore_index=True)\n",
    "full_81_df = full_81_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "full_81_df['result_timestamp'] =full_81_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_81_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_81_df =full_81_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_81_df.reset_index(inplace=True)\n",
    "full_81_df.to_parquet('Dataset/full_81_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read kiosk location useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = '/Users/siucheung/School/Modern Data Analytics/project/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*stadspark.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_kiosk_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_kiosk_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "#full_kiosk_df = full_kiosk_df.resample('15min',closed = 'right', on='result_timestamp').mean()\n",
    "#full_kiosk_df.reset_index(inplace=True)\n",
    "full_kiosk_df.to_parquet('Dataset/full_kiosk_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read hof location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = '/Users/siucheung/School/Modern Data Analytics/project/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*vrijthof.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_hof_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hof_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_hof_df = full_hof_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_hof_df.reset_index(inplace=True)\n",
    "full_hof_df.to_parquet('Dataset/full_hof_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*vrijthof.csv\"))\n",
    "full_hof_df = pd.concat(df_list, ignore_index=True)\n",
    "full_hof_df = full_hof_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "full_hof_df['result_timestamp'] =full_hof_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_hof_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_hof_df =full_hof_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_hof_df.reset_index(inplace=True)\n",
    "full_hof_df.to_parquet('Dataset/full_hof_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read his his-hears location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = '/Users/siucheung/School/Modern Data Analytics/project/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*hears.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_hears_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hears_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_hears_df = full_hears_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_hears_df.reset_index(inplace=True)\n",
    "full_hears_df.to_parquet('Dataset/full_hears_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*hears.csv\") )\n",
    "full_hears_df = pd.concat(df_list, ignore_index=True)\n",
    "full_hears_df = full_hears_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "full_hears_df['result_timestamp'] =full_hears_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_hears_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_hears_df =full_hears_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_hears_df.reset_index(inplace=True)\n",
    "full_hears_df.to_parquet('Dataset/full_hears_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "#TODO: improve, get X number of characters from CSV files\n",
    "\n",
    "def createDataFiles( file_path_end):\n",
    "    '''\n",
    "    Loops over the provided file path ends & retrieves csv files to create parquet files \n",
    "    '''\n",
    "    path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "    \n",
    "    df_list = (pd.read_csv(file, sep = \";\")\n",
    "    for file in glob.glob(path + \"/*/*\"+f'{file_path_end}'+\".csv\") )\n",
    "    df_name = pd.concat(df_list, ignore_index=True)\n",
    "    df_name = df_name[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "    df_name['result_timestamp'] =df_name['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "    df_name.sort_values(by=['result_timestamp'],inplace=True)\n",
    "    df_name =df_name.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "    df_name.reset_index(inplace=True)\n",
    "    df_name.to_parquet(f\"Dataset/full_{file_path_end}_df.parquet\")\n",
    "        \n",
    "    return df_name\n",
    "    # df_name = df\n",
    "\n",
    "df_taste = createDataFiles( file_path_end=\"taste\")\n",
    "# df_vrijthof = createDataFiles(file_path_end=\"vrijthof\")\n",
    "df_hears= createDataFiles(file_path_end=\"hears\")\n",
    "df_kiosk= createDataFiles(file_path_end=\"stadspark\")\n",
    "df_81= createDataFiles(file_path_end=\"81\")\n",
    "df_filosovia= createDataFiles(file_path_end=\"filosovia\")\n",
    "# df_maxim =createDataFiles(file_path_end=\"maxim\")\n",
    "# df_xior = createDataFiles(file_path_end=\"xior\")\n",
    "df_kapel = createDataFiles(file_path_end=\"calvariekapel-ku-leuven\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline\n",
    "used for creating new columns, dealing with the missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test commit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
