{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Maxim location for all the months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*naamsestraat-35-maxim.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_maxim_df = pd.concat(df_list, ignore_index=True)\n",
    "full_maxim_df.shape\n",
    "\n",
    "del df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_maxim_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_maxim_df = full_maxim_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_maxim_df.reset_index(inplace=True)\n",
    "full_maxim_df.to_parquet('Dataset/full_maxim_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_timestamp</th>\n",
       "      <th>lamax</th>\n",
       "      <th>laeq</th>\n",
       "      <th>lceq</th>\n",
       "      <th>lcpeak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-28 08:20:00</td>\n",
       "      <td>58.400905</td>\n",
       "      <td>56.715493</td>\n",
       "      <td>67.821278</td>\n",
       "      <td>79.327515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-28 08:40:00</td>\n",
       "      <td>59.563057</td>\n",
       "      <td>57.756369</td>\n",
       "      <td>65.896815</td>\n",
       "      <td>77.866975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-02-28 09:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-02-28 09:20:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-02-28 09:40:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21401</th>\n",
       "      <td>2022-12-22 14:00:00</td>\n",
       "      <td>55.792917</td>\n",
       "      <td>53.752500</td>\n",
       "      <td>62.573792</td>\n",
       "      <td>75.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21402</th>\n",
       "      <td>2022-12-22 14:20:00</td>\n",
       "      <td>55.173917</td>\n",
       "      <td>53.262000</td>\n",
       "      <td>61.611900</td>\n",
       "      <td>74.158667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21403</th>\n",
       "      <td>2022-12-22 14:40:00</td>\n",
       "      <td>56.143417</td>\n",
       "      <td>54.098583</td>\n",
       "      <td>62.389817</td>\n",
       "      <td>75.100867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21404</th>\n",
       "      <td>2022-12-22 15:00:00</td>\n",
       "      <td>56.173750</td>\n",
       "      <td>54.176333</td>\n",
       "      <td>61.712333</td>\n",
       "      <td>74.699358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21405</th>\n",
       "      <td>2022-12-22 15:20:00</td>\n",
       "      <td>55.518324</td>\n",
       "      <td>53.508090</td>\n",
       "      <td>60.805663</td>\n",
       "      <td>73.957368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21406 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         result_timestamp      lamax       laeq       lceq     lcpeak\n",
       "0     2022-02-28 08:20:00  58.400905  56.715493  67.821278  79.327515\n",
       "1     2022-02-28 08:40:00  59.563057  57.756369  65.896815  77.866975\n",
       "2     2022-02-28 09:00:00        NaN        NaN        NaN        NaN\n",
       "3     2022-02-28 09:20:00        NaN        NaN        NaN        NaN\n",
       "4     2022-02-28 09:40:00        NaN        NaN        NaN        NaN\n",
       "...                   ...        ...        ...        ...        ...\n",
       "21401 2022-12-22 14:00:00  55.792917  53.752500  62.573792  75.210100\n",
       "21402 2022-12-22 14:20:00  55.173917  53.262000  61.611900  74.158667\n",
       "21403 2022-12-22 14:40:00  56.143417  54.098583  62.389817  75.100867\n",
       "21404 2022-12-22 15:00:00  56.173750  54.176333  61.712333  74.699358\n",
       "21405 2022-12-22 15:20:00  55.518324  53.508090  60.805663  73.957368\n",
       "\n",
       "[21406 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxim_df = pd.read_parquet('Dataset/full_maxim_df.parquet')\n",
    "maxim_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selecting night hours + Adding day of the week and holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katri\\AppData\\Local\\Temp\\ipykernel_29228\\2508151511.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  maxim_df.loc[:, 'night_scale'] = (maxim_df['result_timestamp'] - pd.Timedelta(hours=8)).dt.strftime('%d-%m-%Y %H:%M')\n",
      "C:\\Users\\katri\\AppData\\Local\\Temp\\ipykernel_29228\\2508151511.py:4: UserWarning: Parsing dates in %d-%m-%Y %H:%M format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  maxim_df['night_scale'] = pd.to_datetime(maxim_df['night_scale'])\n",
      "C:\\Users\\katri\\AppData\\Local\\Temp\\ipykernel_29228\\2508151511.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  maxim_df['night_scale'] = pd.to_datetime(maxim_df['night_scale'])\n",
      "C:\\Users\\katri\\AppData\\Local\\Temp\\ipykernel_29228\\2508151511.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  maxim_df.loc[:, 'night_hour'] = (maxim_df['night_scale'].dt.hour + maxim_df['night_scale'].dt.minute/60) - 11\n",
      "C:\\Users\\katri\\AppData\\Local\\Temp\\ipykernel_29228\\2508151511.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  maxim_df.loc[:, 'night'] = maxim_df['night_scale'].dt.strftime('%d-%m-%Y')\n",
      "C:\\Users\\katri\\AppData\\Local\\Temp\\ipykernel_29228\\2508151511.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  maxim_df[\"weekday\"] = maxim_df['night_scale'].dt.day_name()\n",
      "C:\\Users\\katri\\AppData\\Local\\Temp\\ipykernel_29228\\2508151511.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  maxim_df['Holiday']=is_holiday.map({True: 1, False:0})\n"
     ]
    }
   ],
   "source": [
    "maxim_df = maxim_df.loc[(maxim_df['result_timestamp'].dt.hour <= 7) | (maxim_df['result_timestamp'].dt.hour >= 19)]\n",
    "\n",
    "maxim_df.loc[:, 'night_scale'] = (maxim_df['result_timestamp'] - pd.Timedelta(hours=8)).dt.strftime('%d-%m-%Y %H:%M')\n",
    "maxim_df['night_scale'] = pd.to_datetime(maxim_df['night_scale'])\n",
    "   #night from monday to tuesday counted as monday\n",
    "maxim_df.loc[:, 'night_hour'] = (maxim_df['night_scale'].dt.hour + maxim_df['night_scale'].dt.minute/60) - 11\n",
    "maxim_df.loc[:, 'night'] = maxim_df['night_scale'].dt.strftime('%d-%m-%Y')\n",
    "\n",
    "maxim_df[\"weekday\"] = maxim_df['night_scale'].dt.day_name()\n",
    "holiday = [\"01-01-2022\", \"18-04-2022\", \"16-05-2022\", \"21-07-2022\", \"25-08-2022\", \"01-11-2022\", \"02-11-2022\",\"11-11-2022\", \"25-12-2022\"]\n",
    "is_holiday = maxim_df['night'].isin(holiday)\n",
    "maxim_df['Holiday']=is_holiday.map({True: 1, False:0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxim_df['result_timestamp'].dt.month.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_df = maxim_df[maxim_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3, 10, 11, 12])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df[\"result_timestamp\"].dt.month.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28, 27, 25, 26, 29, 30, 31,  1,  2, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23,  3,  4,  5,  6,  7,  8,  9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df['result_timestamp'].dt.day.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "# missing_df['result_timestamp'].map(lambda x: datetime.datetime(x.month, max(calendar.monthcalendar(x.year, x.month)[-1][:5])))\n",
    "\n",
    "missing_days_months = missing_df['result_timestamp'].apply(lambda x: x.strftime('%B-%d-´%H:%M')) \n",
    "for (date_time) in missing_days_months:\n",
    "    if datetime.strptime(date_time,'%B-%d-´%H:%M').hour <= 5 or datetime.strptime(date_time,'%B-%d-´%H:%M').hour >=19:\n",
    "        print(date_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  3, 10, 11, 12])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df['result_timestamp'].dt.month.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "oct_df = maxim_df[maxim_df['result_timestamp'].dt.month == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_oct_df = oct_df[oct_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_oct_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read Xior location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*xior.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                     usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser)  \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_xior_df = pd.concat(df_list, ignore_index=True)\n",
    "full_xior_df.shape\n",
    "\n",
    "del df_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above code: error dtype float\n",
    "Below: alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*xior.csv\") )\n",
    "full_xior_df = pd.concat(df_list, ignore_index=True)\n",
    "full_xior_df = full_xior_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_xior_df['result_timestamp'] = full_xior_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_xior_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_xior_df = full_xior_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_xior_df.reset_index(inplace=True)\n",
    "full_xior_df.to_parquet('Dataset/full_xior_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('Dataset/full_xior_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read Taste location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*taste.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_taste_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_taste_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_taste_df = full_taste_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_taste_df.reset_index(inplace=True)\n",
    "full_taste_df.to_parquet('Dataset/full_taste_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*taste.csv\") )\n",
    "full_taste_df = pd.concat(df_list, ignore_index=True)\n",
    "full_taste_df = full_taste_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "\n",
    "full_taste_df['result_timestamp'] = full_taste_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "full_taste_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_taste_df = full_taste_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_taste_df.reset_index(inplace=True)\n",
    "full_taste_df.to_parquet('Dataset/full_taste_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_taste_df = pd.read_parquet('Dataset/full_maxim_df.parquet')\n",
    "full_taste_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read Kapel location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*calvariekapel-ku-leuven.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_kapel_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_kapel_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_kapel_df = full_kapel_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_kapel_df.reset_index(inplace=True)\n",
    "full_kapel_df.to_parquet('Dataset/full_kapel_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m full_kapel_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(df_list, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m full_kapel_df \u001b[39m=\u001b[39m full_kapel_df[[\u001b[39m'\u001b[39m\u001b[39mresult_timestamp\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlamax\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlaeq\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlceq\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlcpeak\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m----> 9\u001b[0m full_kapel_df[\u001b[39m'\u001b[39m\u001b[39mresult_timestamp\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m full_kapel_df[\u001b[39m'\u001b[39;49m\u001b[39mresult_timestamp\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: datetime\u001b[39m.\u001b[39;49mstrptime(x, \u001b[39m'\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mm/\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mY \u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mH:\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mM:\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mS.\u001b[39;49m\u001b[39m%f\u001b[39;49;00m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     11\u001b[0m full_kapel_df\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mresult_timestamp\u001b[39m\u001b[39m'\u001b[39m],inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m full_kapel_df \u001b[39m=\u001b[39mfull_kapel_df\u001b[39m.\u001b[39mresample(\u001b[39m'\u001b[39m\u001b[39m20min\u001b[39m\u001b[39m'\u001b[39m,closed \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mright\u001b[39m\u001b[39m'\u001b[39m, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mresult_timestamp\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Modern-Data-Analytics\\Lib\\site-packages\\pandas\\core\\series.py:4631\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4522\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4523\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4526\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4527\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4528\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4530\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4630\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4631\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Modern-Data-Analytics\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Modern-Data-Analytics\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Modern-Data-Analytics\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[75], line 9\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      5\u001b[0m full_kapel_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(df_list, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m full_kapel_df \u001b[39m=\u001b[39m full_kapel_df[[\u001b[39m'\u001b[39m\u001b[39mresult_timestamp\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlamax\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlaeq\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlceq\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlcpeak\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m----> 9\u001b[0m full_kapel_df[\u001b[39m'\u001b[39m\u001b[39mresult_timestamp\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m full_kapel_df[\u001b[39m'\u001b[39m\u001b[39mresult_timestamp\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: datetime\u001b[39m.\u001b[39;49mstrptime(x, \u001b[39m'\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mm/\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mY \u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mH:\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mM:\u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39mS.\u001b[39;49m\u001b[39m%f\u001b[39;49;00m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     11\u001b[0m full_kapel_df\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mresult_timestamp\u001b[39m\u001b[39m'\u001b[39m],inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m full_kapel_df \u001b[39m=\u001b[39mfull_kapel_df\u001b[39m.\u001b[39mresample(\u001b[39m'\u001b[39m\u001b[39m20min\u001b[39m\u001b[39m'\u001b[39m,closed \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mright\u001b[39m\u001b[39m'\u001b[39m, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mresult_timestamp\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\_strptime.py:568\u001b[0m, in \u001b[0;36m_strptime_datetime\u001b[1;34m(cls, data_string, format)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_strptime_datetime\u001b[39m(\u001b[39mcls\u001b[39m, data_string, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%a\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mb \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS \u001b[39m\u001b[39m%\u001b[39m\u001b[39mY\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    566\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return a class cls instance based on the input string and the\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[39m    format string.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m     tt, fraction, gmtoff_fraction \u001b[39m=\u001b[39m _strptime(data_string, \u001b[39mformat\u001b[39;49m)\n\u001b[0;32m    569\u001b[0m     tzname, gmtoff \u001b[39m=\u001b[39m tt[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]\n\u001b[0;32m    570\u001b[0m     args \u001b[39m=\u001b[39m tt[:\u001b[39m6\u001b[39m] \u001b[39m+\u001b[39m (fraction,)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\_strptime.py:320\u001b[0m, in \u001b[0;36m_strptime\u001b[1;34m(data_string, format)\u001b[0m\n\u001b[0;32m    317\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg\u001b[39m.\u001b[39mformat(index, \u001b[39mtype\u001b[39m(arg)))\n\u001b[0;32m    319\u001b[0m \u001b[39mglobal\u001b[39;00m _TimeRE_cache, _regex_cache\n\u001b[1;32m--> 320\u001b[0m \u001b[39mwith\u001b[39;49;00m _cache_lock:\n\u001b[0;32m    321\u001b[0m     locale_time \u001b[39m=\u001b[39;49m _TimeRE_cache\u001b[39m.\u001b[39;49mlocale_time\n\u001b[0;32m    322\u001b[0m     \u001b[39mif\u001b[39;49;00m (_getlang() \u001b[39m!=\u001b[39;49m locale_time\u001b[39m.\u001b[39;49mlang \u001b[39mor\u001b[39;49;00m\n\u001b[0;32m    323\u001b[0m         time\u001b[39m.\u001b[39;49mtzname \u001b[39m!=\u001b[39;49m locale_time\u001b[39m.\u001b[39;49mtzname \u001b[39mor\u001b[39;49;00m\n\u001b[0;32m    324\u001b[0m         time\u001b[39m.\u001b[39;49mdaylight \u001b[39m!=\u001b[39;49m locale_time\u001b[39m.\u001b[39;49mdaylight):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*calvariekapel-ku-leuven.csv\") )\n",
    "full_kapel_df = pd.concat(df_list, ignore_index=True)\n",
    "full_kapel_df = full_kapel_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "\n",
    "full_kapel_df['result_timestamp'] = full_kapel_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_kapel_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_kapel_df =full_kapel_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_kapel_df.reset_index(inplace=True)\n",
    "full_kapel_df.to_parquet('Dataset/full_kapel_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read filosovia location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = '/Users/siucheung/School/Modern Data Analytics/project/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*filosovia.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_filo_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_filo_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_filo_df = full_filo_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_filo_df.reset_index(inplace=True)\n",
    "full_filo_df.to_parquet('Dataset/full_filo_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*filosovia.csv\"))\n",
    "full_filo_df = pd.concat(df_list, ignore_index=True)\n",
    "full_filo_df = full_filo_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "full_filo_df['result_timestamp'] =full_filo_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_filo_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_filo_df =full_filo_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_filo_df.reset_index(inplace=True)\n",
    "full_filo_df.to_parquet('Dataset/full_filo_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read 81 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = '/Users/siucheung/School/Modern Data Analytics/project/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*81.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_81_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_81_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_81_df = full_81_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_81_df.reset_index(inplace=True)\n",
    "full_81_df.to_parquet('Dataset/full_81_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*81.csv\"))\n",
    "full_81_df = pd.concat(df_list, ignore_index=True)\n",
    "full_81_df = full_81_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "full_81_df['result_timestamp'] =full_81_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_81_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_81_df =full_81_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_81_df.reset_index(inplace=True)\n",
    "full_81_df.to_parquet('Dataset/full_81_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read kiosk location useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = '/Users/siucheung/School/Modern Data Analytics/project/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*stadspark.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_kiosk_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_kiosk_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "#full_kiosk_df = full_kiosk_df.resample('15min',closed = 'right', on='result_timestamp').mean()\n",
    "#full_kiosk_df.reset_index(inplace=True)\n",
    "full_kiosk_df.to_parquet('Dataset/full_kiosk_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read hof location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = '/Users/siucheung/School/Modern Data Analytics/project/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*vrijthof.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_hof_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hof_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_hof_df = full_hof_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_hof_df.reset_index(inplace=True)\n",
    "full_hof_df.to_parquet('Dataset/full_hof_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*vrijthof.csv\"))\n",
    "full_hof_df = pd.concat(df_list, ignore_index=True)\n",
    "full_hof_df = full_hof_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "full_hof_df['result_timestamp'] =full_hof_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_hof_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_hof_df =full_hof_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_hof_df.reset_index(inplace=True)\n",
    "full_hof_df.to_parquet('Dataset/full_hof_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read his his-hears location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = '/Users/siucheung/School/Modern Data Analytics/project/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*hears.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_hears_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hears_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_hears_df = full_hears_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_hears_df.reset_index(inplace=True)\n",
    "full_hears_df.to_parquet('Dataset/full_hears_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*hears.csv\") )\n",
    "full_hears_df = pd.concat(df_list, ignore_index=True)\n",
    "full_hears_df = full_hears_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "full_hears_df['result_timestamp'] =full_hears_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_hears_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_hears_df =full_hears_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_hears_df.reset_index(inplace=True)\n",
    "full_hears_df.to_parquet('Dataset/full_hears_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "#TODO: improve, get X number of characters from CSV files\n",
    "\n",
    "def createDataFiles(file_path_end = []):\n",
    "    '''\n",
    "    Loops over the provided file path ends & retrieves csv files to create parquet files \n",
    "    '''\n",
    "    path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "    for location in file_path_end:\n",
    "        df_name = pd.DataFrame()\n",
    "\n",
    "        df_list = (pd.read_csv(file, sep = \";\")\n",
    "                for file in glob.glob(path + \"/*/*\"+f'{location}'+\".csv\") )\n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "        df = df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "        df['result_timestamp'] =df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "        df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "        df =df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "        df.reset_index(inplace=True)\n",
    "        df.to_parquet(f\"Dataset/full_{file_path_end}_df.parquet\")\n",
    "    # df_name = df\n",
    "\n",
    "createDataFiles(file_path_end= \"maxim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('Dataset/full_maxim_df.parquet')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline\n",
    "used for creating new columns, dealing with the missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test commit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
