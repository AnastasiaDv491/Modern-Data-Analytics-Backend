{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Maxim location for all the months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*naamsestraat-35-maxim.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_maxim_df = pd.concat(df_list, ignore_index=True)\n",
    "full_maxim_df.shape\n",
    "\n",
    "del df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_maxim_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_maxim_df = full_maxim_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_maxim_df.reset_index(inplace=True)\n",
    "full_maxim_df.to_parquet('Dataset/full_maxim_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read Xior location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*xior.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                     usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser)  \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_xior_df = pd.concat(df_list, ignore_index=True)\n",
    "full_xior_df.shape\n",
    "\n",
    "del df_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above code: error dtype float\n",
    "Below: alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*xior.csv\") )\n",
    "full_xior_df = pd.concat(df_list, ignore_index=True)\n",
    "full_xior_df = full_xior_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_xior_df['result_timestamp'] = full_xior_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_xior_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_xior_df = full_xior_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_xior_df.reset_index(inplace=True)\n",
    "full_xior_df.to_parquet('Dataset/full_xior_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet('Dataset/full_xior_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read Taste location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*taste.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_taste_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_taste_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_taste_df = full_taste_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_taste_df.reset_index(inplace=True)\n",
    "full_taste_df.to_parquet('Dataset/full_taste_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*taste.csv\") )\n",
    "full_taste_df = pd.concat(df_list, ignore_index=True)\n",
    "full_taste_df = full_taste_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "\n",
    "full_taste_df['result_timestamp'] = full_taste_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "full_taste_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_taste_df = full_taste_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_taste_df.reset_index(inplace=True)\n",
    "full_taste_df.to_parquet('Dataset/full_taste_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_taste_df = pd.read_parquet('Dataset/full_maxim_df.parquet')\n",
    "full_taste_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read Kapel location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*calvariekapel-ku-leuven.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_kapel_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_kapel_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_kapel_df = full_kapel_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_kapel_df.reset_index(inplace=True)\n",
    "full_kapel_df.to_parquet('Dataset/full_kapel_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*calvariekapel-ku-leuven.csv\") )\n",
    "full_kapel_df = pd.concat(df_list, ignore_index=True)\n",
    "full_kapel_df = full_kapel_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "\n",
    "full_kapel_df['result_timestamp'] = full_kapel_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_kapel_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_kapel_df =full_kapel_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_kapel_df.reset_index(inplace=True)\n",
    "full_kapel_df.to_parquet('Dataset/full_kapel_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read filosovia location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = '/Users/siucheung/School/Modern Data Analytics/project/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*filosovia.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_filo_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_filo_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_filo_df = full_filo_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_filo_df.reset_index(inplace=True)\n",
    "full_filo_df.to_parquet('Dataset/full_filo_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*filosovia.csv\"))\n",
    "full_filo_df = pd.concat(df_list, ignore_index=True)\n",
    "full_filo_df = full_filo_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "full_filo_df['result_timestamp'] =full_filo_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_filo_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_filo_df =full_filo_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_filo_df.reset_index(inplace=True)\n",
    "full_filo_df.to_parquet('Dataset/full_filo_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read 81 location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = '/Users/siucheung/School/Modern Data Analytics/project/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*81.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_81_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_81_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_81_df = full_81_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_81_df.reset_index(inplace=True)\n",
    "full_81_df.to_parquet('Dataset/full_81_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*81.csv\"))\n",
    "full_81_df = pd.concat(df_list, ignore_index=True)\n",
    "full_81_df = full_81_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "full_81_df['result_timestamp'] =full_81_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_81_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_81_df =full_81_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_81_df.reset_index(inplace=True)\n",
    "full_81_df.to_parquet('Dataset/full_81_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read kiosk location useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = '/Users/siucheung/School/Modern Data Analytics/project/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*stadspark.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_kiosk_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_kiosk_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "#full_kiosk_df = full_kiosk_df.resample('15min',closed = 'right', on='result_timestamp').mean()\n",
    "#full_kiosk_df.reset_index(inplace=True)\n",
    "full_kiosk_df.to_parquet('Dataset/full_kiosk_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read hof location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = '/Users/siucheung/School/Modern Data Analytics/project/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*vrijthof.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_hof_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hof_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_hof_df = full_hof_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_hof_df.reset_index(inplace=True)\n",
    "full_hof_df.to_parquet('Dataset/full_hof_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*vrijthof.csv\"))\n",
    "full_hof_df = pd.concat(df_list, ignore_index=True)\n",
    "full_hof_df = full_hof_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "full_hof_df['result_timestamp'] =full_hof_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_hof_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_hof_df =full_hof_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_hof_df.reset_index(inplace=True)\n",
    "full_hof_df.to_parquet('Dataset/full_hof_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read his his-hears location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_date_parser = lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f')\n",
    "path = '/Users/siucheung/School/Modern Data Analytics/project/Full Data Set/'\n",
    "\n",
    "csv_files = glob.glob(path + \"/*/*hears.csv\") \n",
    "df_list = (pd.read_csv(file, sep=';',\n",
    "                       usecols=['result_timestamp','lamax','laeq','lceq','lcpeak'],\n",
    "                       dtype={'lamax':'float16','laeq':'float16','lceq':'float16','lcpeak':'float16'},\n",
    "                       parse_dates=['result_timestamp'],\n",
    "                       date_parser=custom_date_parser) \n",
    "                       for file in csv_files)\n",
    "\n",
    "full_hears_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_hears_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_hears_df = full_hears_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_hears_df.reset_index(inplace=True)\n",
    "full_hears_df.to_parquet('Dataset/full_hears_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "\n",
    "df_list = (pd.read_csv(file, sep = \";\")\n",
    "           for file in glob.glob(path + \"/*/*hears.csv\") )\n",
    "full_hears_df = pd.concat(df_list, ignore_index=True)\n",
    "full_hears_df = full_hears_df[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "full_hears_df['result_timestamp'] =full_hears_df['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "full_hears_df.sort_values(by=['result_timestamp'],inplace=True)\n",
    "full_hears_df =full_hears_df.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "full_hears_df.reset_index(inplace=True)\n",
    "full_hears_df.to_parquet('Dataset/full_hears_df.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "#TODO: improve, get X number of characters from CSV files\n",
    "\n",
    "def createDataFiles( file_path_end):\n",
    "    '''\n",
    "    Loops over the provided file path ends & retrieves csv files to create parquet files \n",
    "    '''\n",
    "    path = 'C:/Users/nastj/Downloads/Full Data Set/'\n",
    "    \n",
    "    df_list = (pd.read_csv(file, sep = \";\")\n",
    "    for file in glob.glob(path + \"/*/*\"+f'{file_path_end}'+\".csv\") )\n",
    "    df_name = pd.concat(df_list, ignore_index=True)\n",
    "    df_name = df_name[['result_timestamp','lamax','laeq','lceq','lcpeak']]\n",
    "\n",
    "    df_name['result_timestamp'] =df_name['result_timestamp'].apply(lambda x: datetime.strptime(x, '%d/%m/%Y %H:%M:%S.%f'))\n",
    "\n",
    "    df_name.sort_values(by=['result_timestamp'],inplace=True)\n",
    "    df_name =df_name.resample('20min',closed = 'right', on='result_timestamp').mean()\n",
    "    df_name.reset_index(inplace=True)\n",
    "    df_name.to_parquet(f\"Dataset/full_{file_path_end}_df.parquet\")\n",
    "        \n",
    "    return df_name\n",
    "    # df_name = df\n",
    "\n",
    "df_taste = createDataFiles( file_path_end=\"taste\")\n",
    "# df_vrijthof = createDataFiles(file_path_end=\"vrijthof\")\n",
    "df_hears= createDataFiles(file_path_end=\"hears\")\n",
    "df_kiosk= createDataFiles(file_path_end=\"stadspark\")\n",
    "df_81= createDataFiles(file_path_end=\"81\")\n",
    "df_filosovia= createDataFiles(file_path_end=\"filosovia\")\n",
    "# df_maxim =createDataFiles(file_path_end=\"maxim\")\n",
    "# df_xior = createDataFiles(file_path_end=\"xior\")\n",
    "df_kapel = createDataFiles(file_path_end=\"calvariekapel-ku-leuven\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline\n",
    "used for creating new columns, dealing with the missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test commit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
